# Emby Server Configuration
EMBY_SERVER_URL=http://localhost:8096
EMBY_API_KEY=your-emby-api-key-here
EMBY_USER_ID=your-emby-user-id-here

# AI Provider Configuration
AI_PROVIDER=lmstudio
#AI_PROVIDER="ollama"
#AI_PROVIDER="api"

# LM Studio Configuration (for local AI model)
LMSTUDIO_MODEL_NAME=qwen2.5-vl-7b-instruct-abliterated
LMSTUDIO_MAX_CONCURRENT=2

# Ollama Configuration (if using Ollama)
OLLAMA_MODEL_NAME=qwen2.5vl:3b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MAX_CONCURRENT=1

# Z.AI API Configuration (if using API)
API_MODEL_NAME=glm-4.5v
API_BASE_URL=https://api.z.ai/api/paas/v4/chat/completions
API_AUTH_TOKEN=your-zai-api-key-here
API_MAX_CONCURRENT=3

# Path Mappings
# Format: /source_path:/destination_path
PATH_MAPPINGS=/volume1/shows:/Volumes/shows,/volume1/movies:/Volumes/movies

# Processing Configuration
DAYS_BACK=5
PROCESS_FAVORITES=false
FAVORITES_ONLY=false
MAX_CONCURRENT_VIDEOS=2
MAX_PIXELS=640000

# Image Processing Configuration
# Maximum pixel count for extracted frames (default: 640000 = 800x800)
# Frames larger than this will be resized to fit within the limit while maintaining aspect ratio
# This helps optimize LLM processing and reduce memory usage

# Parallel Processing Configuration
# Number of concurrent requests for frame analysis (higher values = faster processing but more resource usage)
# LMSTUDIO_MAX_CONCURRENT: Default 2 (local processing, adjust based on your system resources)
# OLLAMA_MAX_CONCURRENT: Default 1 (Ollama has built-in rate limiting)
# API_MAX_CONCURRENT: Default 3 (cloud API, can be increased for better throughput)
# MAX_CONCURRENT_VIDEOS: Default 2 (number of videos to process in parallel, adjust based on your system resources)

# Favorites Copy Configuration
# Copy favorite videos to this folder (leave empty to disable)
COPY_FAVORITES_TO=